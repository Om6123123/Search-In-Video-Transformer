{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from google.colab.patches import cv2_imshow\n",
        "from moviepy.editor import VideoFileClip\n",
        "import torchaudio\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Initialize Wav2Vec2 model and processor\n",
        "def load_model_and_processor(model_name):\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "    model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "    return model, processor\n",
        "\n",
        "# Function to extract audio from video\n",
        "def extract_audio(video_file, audio_file):\n",
        "    video = VideoFileClip(video_file)\n",
        "    audio = video.audio\n",
        "    audio.write_audiofile(audio_file)\n",
        "\n",
        "# Function to transcribe audio using Wav2Vec2\n",
        "def transcribe_audio(model, processor, audio_file):\n",
        "    waveform, sample_rate = torchaudio.load(audio_file)\n",
        "\n",
        "    # Ensure the waveform is a 2D tensor: [channel, samples]\n",
        "    if waveform.ndim == 1:\n",
        "        waveform = waveform.unsqueeze(0)\n",
        "\n",
        "    # Resample to 16kHz if necessary\n",
        "    if sample_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Wav2Vec2 expects a single channel (mono) input\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Process the waveform\n",
        "    inputs = processor(waveform.squeeze(), return_tensors=\"pt\", sampling_rate=16000)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "\n",
        "    predicted_ids = torch.argmax(logits, dim=-1)\n",
        "    transcription = processor.batch_decode(predicted_ids)\n",
        "    return transcription[0]\n",
        "\n",
        "# Function to find the timestamp for a specific word\n",
        "def find_word_timestamp(word, transcription):\n",
        "    words = transcription.split()\n",
        "    print(words)\n",
        "    timestamps = []\n",
        "    for i, w in enumerate(words):\n",
        "        if word.lower() in w.lower():\n",
        "            timestamps.append(i)  # Simplified; no exact timestamp here\n",
        "    print(timestamps)\n",
        "    return timestamps\n",
        "\n",
        "# Function to skip to a specific time in the video\n",
        "def skip_to_time(video_file, timestamp):\n",
        "    video = cv2.VideoCapture(video_file)\n",
        "    fps = video.get(cv2.CAP_PROP_FPS)\n",
        "    frame_number = int(fps * timestamp)\n",
        "\n",
        "    video.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
        "\n",
        "    while video.isOpened():\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        # cv2_imshow(frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    video.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Main function to search for a word in the video\n",
        "def search_in_video_transformer(video_file, audio_file, model_name, search_word):\n",
        "    # Load Wav2Vec2 model and processor\n",
        "    model, processor = load_model_and_processor(model_name)\n",
        "\n",
        "    # Extract audio from video\n",
        "    extract_audio(video_file, audio_file)\n",
        "\n",
        "    # Transcribe audio\n",
        "    transcription = transcribe_audio(model, processor, audio_file)\n",
        "    print(f\"Transcription: {transcription}\")\n",
        "\n",
        "    # Find timestamp for the specific word\n",
        "    timestamps = find_word_timestamp(search_word, transcription)\n",
        "\n",
        "    if timestamps:\n",
        "        # For simplicity, assume the timestamp of the first found word\n",
        "        skip_to_time(video_file, timestamps[0])\n",
        "    else:\n",
        "        print(\"Word not found in the video.\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Replace with your own file paths and word to search\n",
        "    video_file = '/content/videoplayback.mp4'  # Upload this file\n",
        "    audio_file = 'extracted_audio.wav'\n",
        "    model_name = 'facebook/wav2vec2-large-960h-lv60-self'  # Pre-trained model for English accents\n",
        "    search_word = 'tackling'\n",
        "\n",
        "    search_in_video_transformer(video_file, audio_file, model_name, search_word)"
      ],
      "metadata": {
        "id": "ngp7tUqQutP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nr0NI4F00khv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}